# 机器学习笔记





## 分类

### 监督学习

根据大量正确的 X 到 Y 的答案，使模型计算出 X -> Y 的映射

**回归**

预测随机数字

**分类**

预测类别 

**PS**：以线性回归为例，虽然函数形式是预先定义的，如$y=w*x+b$，但具体参数并不是手动设置的，而是通过数据中学习得到的

### 非监督学习

模型自行计算出数据可能有的模式或结构

**聚类**

将未标记的数据放入不同集群

**异常检测**

**降维** 

尽可能少损失信息地压缩一组数据



## 线性回归

根据训练集拟合一个 $f_{w,b}(x)=wx+b$ 的函数，同时需要构建一个形如
$$
J_{(w,b)}= \frac{1}{2m}\sum^m_{i=1}(\hat y^{(i)}-y^{(i)})^2 \\
m=训练样本个数，\hat y = f_{w,b}(x)
$$
的成本函数（平方误差成本函数），

其中，函数 J 的值将根据所选参数 w 、b 的值呈现出不同的值，从而根据最低的成本（ J 尽可能小）拟合出一个最接近数据集的一次函数



#### 多类特征

当预测一个目标需要考虑多类特征（如价格、位置等影响房价）$x_1,x_2,x_3,...,x_n $ 时，构造方程 $f_{w,b}(x)=w_1x_1+w_2x_2+···+w_nx_n+b$ 

## 梯度下降

一种可用于尝试最小化任何函数的算法，使用了反向传播（逐层计算损失函数对各参数的偏导数，计算梯度）

通过迭代调整参数，找到函数的局部最小值或全局最小值

其核心思想是：通过沿着**最陡峭**的下降方向逐步接近函数的局部或全局最小值。



#### 如何决定下降的步幅？

以线性回归 $\,y=wx+b\,$为例，使用参数更新 
$$
\begin{flalign*}
& tmp\_w =w-\alpha\frac{\partial}{\partial w}J(w,b) \\\\
& tmp\_b =b-\alpha\frac{\partial}{\partial b}J(w,b) \\\\
& w=tmp\_w \\\\
& b=tmp\_b \\\\
\end{flalign*}
$$
其中$\, \alpha\,$表示学习率，越大将对应越激进的下降幅度

越接近 $J(w,b)$ 最小值，由于导数自动变小，步幅将自动放缓



#### 批量梯度下降

在梯度下降的每一步中，同时查看所有的训练示例

#### 随机梯度下降



#### 特征缩放

为什么要使用特征缩放

**加速收敛**

当实行梯度下降时，特征缩放可以加快收敛速度，

当一个特征数值很大时，对其进行梯度更新时对应的梯度也会较大，导致权重 $w_1$ 的更新步长过大，会导致其在该特征方向上收敛的过快，而当另一个特征数值很小时，收敛又过慢，由此会导致**震荡**甚至无法收敛。

通过标准化、归一化等缩放手段，可以让特征值当取值范围相似，确保梯度下降过程中，模型在各个权重的更新速度更加均匀，从而加速模型的收敛

**保持模型稳定性**

若不进行标准化而直接学习较小的权重，会导致权重的数量级差异过大，将导致模型的数值不稳定，可能引发溢出或下溢

**权重大小的直观解释**

标准化后权重大小可以直接反应各个特征值对目标变量的贡献



**常用方法**

1.标准化 ( Standardization )

将特征取值缩放为均值为0，方差为1的标准正态分布，公式为：
$$
x'=\frac{x-\mu}{\sigma}
$$
其中，$\mu$ 是均值，$\sigma$ 是标准差。标准化后，特征的取值范围会集中在 -1 到 1之间。

2.归一化 ( Normalization )

将特征缩放到一个固定范围内。常见的归一化公式为：
$$
x'=\frac{x-x_{min}}{x_{max}-x_{min}}
$$
这种方法适用于特征范围有界且非负的情况



## Logistic 回归

逻辑回归主要用于二分类

定义：输入一组或多组特征 x,输出一个介于0～1的数字，表示结果为真值的可能性
$$
f_{\vec w,b}(\vec x) = g(\underbrace{\vec w·\vec x+b}_{z})=\frac{1}{1+e^{-z}}=\frac{1}{1+e^{-(\vec w·\vec x+b)}}
=P(y=1 \mid \vec x;\vec w,b)
$$
**损失函数**

逻辑回归使用的代价函数称为**对数损失函数**（loss function）,主要表示当预测结果与实际标签不一致时的损失，是一种惩罚机制。

公式为：
$$
L(y,\hat y)=-(y(-log(\hat y))+(1-y)log(1-\hat y))
$$
其中：

$y$ 是实际的标签（0 或 1），$\hat y$ 是模型预测的概率

这个损失函数的目标是最小化预测概率和实际标签之间的差异。



## 过拟合/高方差问题

过拟合问题是指模型在训练集上表现良好，但在未见过的数据上表现较差的现象（无法很好的应用在新数据上）

**原因**

特征数量过多，模型过于复杂，样本中有大量噪声都有可能导致过拟合

**解决方法**

增加训练数据，减少特征，正则化

**正则化**

正则化旨在鼓励模型收缩参数数值以减少其影响，而不必要求参数设置正好为 0（消灭特征）

正则化在代价函数后加入所有参数 $w$ 的平方和以惩罚模型使用较大参数的行为
$$
J(\vec w,b)=\frac{1}{2m}\sum^m_{i=1}(f_{\vec w,b}(\vec x^{(i)})-y^{(i)})^2 + 
\frac{\lambda}{2m}\sum_{j=1}^nw_j^2
$$
其中 $\lambda$ 是指正则参数，当其过小会导致过拟合，过大会导致参数全部接近 0 而造成模型不匹配









## 神经网络

神经网络（Neural Network）是一种模仿生物神经系统工作原理的计算模型，

由大量互相连接的节点（称为“神经元”）组成，这些节点按层次结构排列，通常包括输入层、隐藏层和输出层。



一个 $n$ 层的神经网络一般由以下层组成

1. **输入层（第 0 层）**：接收输入数据，每个节点通常对应输入数据的一个特征。

2. **隐藏层（第 1～n-1 层）**：在输入层和输出层之间，包含一系列的神经元。隐藏层的神经元会对输入数据进行处理，并通过权重和激活函数将数据进行非线性变换，从而提取特征。上一层得出的向量作为下一层的输入

   * **线性变换**：输入数据与该层的权重矩阵相乘，再加上偏置项：
     $$
     Z^{(l)}=W^{(l)}·A^{(l-1)}+b^{(l)}
     $$
     其中，$W^{(l)}$ 的每层对应每个神经元对上一层的输入数据的权重

   * **激活函数**：线性变换结果会经过一个激活函数，目的是引入非线性，使网络能够学习和表示复杂的关系

3. **输出层（第 n 层）**：生成最终结果，可以是分类标签、回归值或其他形式的输出。



### 激活函数

激活函数的作用是引入**非线性**



#### $ReLU$ 函数

函数为：$f(x) = max(0, x)$

神经网络的激活层推荐使用 $ReLU$ 函数而非 $sigmoid$ 或线形函数

使用 $sigmoid$ 函数会导致以下问题：

1. **梯度消失**：

   * **Sigmoid 函数** 的输出范围在 (0, 1) 之间，这会导致深层神经网络中出现梯度消失问题。在反向传播过程中，Sigmoid 函数的导数在输入较大或较小时接近 0，导致梯度在逐层传递时不断减小，使得模型无法有效训练。

   * **ReLU** 的导数为 1（输入为正数时），且不会像 Sigmoid 那样在正向传播时将输入压缩到很小的范围，因此能够减缓梯度消失问题，使网络在训练过程中更稳定。

2. **计算低效**：
   * **Sigmoid 函数**为指数函数，计算速度慢，$ReLU $ 在大型神经网络中更有优势
   
3. **泛化能力弱**：

   * **ReLU** 函数的输出具有稀疏性（输入值为负时输出为 0 ）。这意味着网络中的部分神经元在特定情况下可能不激活，有助于减少神经网络中的依赖性，提升泛化能力
   * **Sigmoid** 函数的输出总是非零的，几乎所有神经元都会激活，从而增加了计算量和过拟合的风险。





#### $Softmax$ 函数

Softmax 常用于神经网络的最后一层输出，用来处理**多分类**问题。

公式如下：
$$
softmax (z_i)= \frac{z_i}{\sum_j e^{(z_j)}}
$$

#### 误差避免



### $Adam$ 算法







## 卷积神经网络

卷积神经网络是基于空间不变性运行的



### 卷积

卷积操作的本质是对**输入数据的每个通道和对应的卷积核通道相乘后相加，填入特征图的相应位置**，即有效提取相邻像素间的相关特征

输入的通道的相同位置的数据是不相同的，同理卷积核的各个通道的相同位置的值也有可能是不同的

**1*1 卷积层**

1*1卷积层没有提取相关特征的功能，然而其唯一发生的计算在**通道**上

其作用是降低通道数



### 通道

通道表示的是输入数据的“深度”，与其颜色维度等特征维度相关

同时，卷积操作会产生特征图，每一层的特征图数量被称为该层的“输出通道”

输出通道控制了模型的表达能力

**输入通道数**

输入通道通常取决于数据的类型

* 对于 RGB 图像，输入通道通常是 3

* 灰度图像则是 1

* 对于网络中的每一层，输入通道数通常是上一层的输出数

**逐层设计**：常见的做法是从一个简单的基础网络开始（例如一个小型卷积网络），然后根据实验调整每一层的通道数。常见的做法是逐渐增大卷积层的输出通道数，尤其是在网络越深时。

**通过网络结构经验**：一些经典的网络结构（如VGG、ResNet、DenseNet等）可以提供通道数的设计思路。例如，VGG网络通常会逐渐增加卷积层的输出通道数，而ResNet网络则通过残差连接来避免通道数过大时的梯度消失问题。

**多输出通道**

可以将每个通道当作对不同特征的响应







### 汇聚层

汇聚层的目的是：**降低卷积层对位置的敏感性，降低对空间采样表示对敏感性**

**主要作用**：对输入特征图进行降采样（减少空间尺寸），从而降低计算复杂度和内存消耗并保持关键特征信息

，提高模型鲁棒性

这种作用可以从以下两个角度来看：

**减轻计算负担**

随着网络的加深，特征图的尺寸和通道数会急剧增大，导致计算量和存储需求迅速上升

因此通过汇聚层进行下采样，减小特征图的尺寸是很有必要的

**丢弃部分信息**

汇聚层会丢失一些信息，但这种信息通常是有"选择性"的，能保留大多数有用的（对分类任务重要的）特征，去掉一些不那么重要的噪声或冗余

这种“丢弃”操作不是完全无意义的，适当的下采样能够提高模型的鲁棒性和泛化能力，即适度的信息丢失有时能过帮助避免过拟合，尤其是训练集较小的情况



##### **汇聚层分类**

| 特性               | 最大汇聚 (Max Pooling)                         | 平均汇聚 (Average Pooling)                                 |
| :----------------- | ---------------------------------------------- | ---------------------------------------------------------- |
| **操作**           | 选择局部区域中的最大值                         | 计算局部区域内所有元素的平均值                             |
| **输出特征**       | 更注重突出、显著的特征（如边缘、角点）         | 更平滑，保留更多的区域信息                                 |
| **对噪声的敏感度** | 对噪声较敏感，因为会选择最大值                 | 对噪声较不敏感，能够平滑局部特征                           |
| **性能**           | 通常能提取到更有用的特征（有时会丢失一些信息） | 更加平衡，有时会丢失更多的细节信息，但能更好地保持整体信息 |
| **应用场景**       | 特别适用于图像识别任务（如边缘检测、物体检测） | 更多用于需要整体特征表示的任务（如分类任务）               |



## AlexNet

在 AlexNet 中共8层

| 11*11 卷积层 | 3*3 最大汇聚层 | 5*5 卷积层 | 3*3 最大汇聚层 | 3*3 卷积层 | 3*3 卷积层 | 3*3 卷积层 | 3*3    最大汇聚层 | 全连接层 | 全连接层 | 全连接层 |
| :----------- | -------------- | ---------- | -------------- | ---------- | ---------- | ---------- | ----------------- | -------- | -------- | -------- |



## VGG

VGG 为基于块的神经网络

**更小的卷积核**

对比 AlexNet ，VGG 使用小卷积核（3*3），减少了运算的参数且性能更好

**卷积块**

VGG 将多个卷积块堆叠在一起形成卷积块（如两个3*3卷积层+一个池化层），这种设计能够在减小计算量的同时保证模型的**感受野**

**灵活的网络复杂度调整**

卷积块可以在模型中复用，以灵活调整模型的复杂度









